# 利用 Kubernetes 和 OpenShift 进行自动化性能测试(第 1 部分)

> 原文:[https://developers . red hat . com/blog/2018/11/22/automated-performance-testing-kubernetes-open shift](https://developers.redhat.com/blog/2018/11/22/automated-performance-testing-kubernetes-openshift)

这是基于我在 EMEA 红帽技术交流中心举行的一次会议的三篇文章系列中的第一篇。在这第一篇文章中，我介绍了利用 [Red Hat OpenShift](https://www.openshift.com) 或 [Kubernetes](https://developers.redhat.com/topics/kubernetes/) 进行自动化性能测试的基本原理和方法，给出了设置的概述，并讨论了在执行和分析性能测试时值得考虑的要点。我还将谈一谈性能调优。

在第二篇文章中，我们将着眼于构建一个可观察性堆栈，除了它在生产中提供的支持之外，它还可以在性能测试中加以利用。像 Prometheus、Jaeger、Elasticsearch 和 Grafana 这样的开源项目将用于这个目的。第三篇文章将介绍用 JMeter 和 Jenkins 构建性能测试环境和自动化执行的细节。

## 基本原理

越来越多的公司正在投入大量精力来缩短将产品创新推向市场或适应市场和监管变化所需的时间。这引发了人们对微服务架构和 T2 开发实践的兴趣。

更频繁的发布听起来不错，但也带来了许多挑战。对于每月、每周或每天的发布，在将代码发布到产品中时，避免破坏是至关重要的。测试是建立信心的主要方法，即代码可以发布了。然而，一些类型的测试传统上需要几周到几个月的人工努力，这对于我们现在谈论的发布速度来说是不可持续的。自动化正变得至关重要。

历史上，通过使用在应用程序构建时运行的单元测试，功能测试具有相当好的自动化水平。非功能集成和性能测试的情况并非如此。尽管本文关注的是性能方面，但是这种方法和设置可以在非功能和集成方面重用。

## 设置概述

此图显示了设置过程的概述:

[![Overview of the setup process](../Images/53633887a634e841c23285eb5a611f5f.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc69ff84eb6.png)

### 应用方面

我用于演示的应用程序是一个非常简单的 Camel route，它从一个队列消费消息，然后将它们转发到另一个队列。应用程序可以配置为注入一些负载和延迟，但真正有趣的部分不是应用程序本身，而是它周围的一切。

我决定使用异步通信，因为我看到越来越多的客户开始采用微服务和事件驱动设计。这也很有趣，因为大多数 JMeter 和 OpenTracing 示例和文档都关注同步调用。将异步性引入其中会使事情变得稍微复杂一些。

所有用于复制设置的应用程序代码和说明都可以在 [my Github repository](https://github.com/fgiloux/auto-perf-test) 中找到。

### 配置外部化

容器图像的一个重要特征和主要好处是它们是不可变的。这极大地简化了发布过程，并降低了其中固有的风险水平。关于测试，不变性也使得保证已经被验证的东西就是被提升到下一个环境的东西变得容易:例如，从集成到 UAT 到产品化。

要做到这一点，从应用程序和容器映像中删除运行时环境的特定细节是很重要的，比如数据库连接信息、消息代理地址或凭证。在 Kubernetes 和 OpenShift (Red Hat 的 Kubernetes 企业发行版)中，通过使用 configMaps 和 secrets，这些细节可以很容易地在外部实现。配置图和秘密中的数据可以作为环境变量或者作为装入容器文件系统的文件注入到运行中的[容器](https://developers.redhat.com/blog/category/containers/)中。两者的工作方式相似，但是 secrets 有额外的限制来保证敏感信息的机密性。

但是应用程序调优参数呢？我指的是消息消费者的数量、连接池的大小等。这些参数会对生产中应用程序的行为产生很大的影响，这意味着将它们隐藏在不可变的容器映像中。请记住:我们希望推广我们已经测试过的产品。另一方面，不能在性能测试期间修改它们(我们需要重新创建映像)可能会降低速度或者减少在一段时间内可以运行的测试的广度。

我认为有两种选择:

*   第一种选择是遵循不可变原则，让调优参数成为映像的一部分。这需要一个更复杂的[持续集成](https://developers.redhat.com/blog/category/ci-cd/)管道来支持。该管道将创建几个具有不同调整参数的图像。
*   第二种选择更加务实。它还包括将这些参数外化到配置映射中。为了降低发布与已测试内容不同的内容的风险，应该在版本控制系统(如 git 或 CMDB)中记录配置映射信息的来源，并为每个发布进行标记。我建议不要将这些文件直接放在代码中。将它们放在不同的存储库中有助于“提升”配置，就像我们提升我们的代码，而不需要在只有配置发生变化时创建新的代码发布一样。为每个环境(集成、UAT、生产)建立单独的存储库，可以让我们清楚地了解每个环境中运行的是什么版本，并轻松地将代码从一个环境升级到另一个环境。

### 可观察性

[![Elasticsearch logo](../Images/351d097691dd4cf11a41ffb3d0b6568d.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc695fb2e2c.png)

运行自动化性能测试是一件很棒的事情，但是为了让它们发挥全部价值，我们需要理解应用程序在负载下的行为。利用为生产准备而构建的可观察性特性是获得这种洞察力的一种直接方式:识别瓶颈、错误状态、负载下的资源消耗等。为此可以使用三个支柱:

*   例如，可以通过 JMX/Jolokia 或 Prometheus 端点收集应用程序指标
*   由于 OpenTracing 和 Jaeger，可以捕获应用程序跟踪/性能
*   日志，使用 OpenShift 可以自动聚合到 Elasticsearch 中，当应用程序将日志写入标准输出时，就可以在 Kibana 中进行查询和报告

我也成功地使用了一些涵盖前两点的商业产品。然而，作为开源项目，Prometheus 和 Jaeger 拥有从不止一家公司获得想法、创新和承诺的优势。

在我的第二篇文章中，我们将更仔细地看看可观察层的设置。

### 经纪人和外部服务

[![EnMasse logo](../Images/d9f9cb4687ba97f5d36c4fbe9e4f85c1.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc68afb5419.png)

应用程序依赖基础设施来完成通常的功能，比如这里的消息代理。负载平衡或状态持久化是其他的例子。这些系统带来的挑战是它们经常被共享。因此，性能测试的结果可能会受到外部因素的影响。文件系统读写和网络通信也可能受到影响。但是，有一些缓解策略:

*   我们可以监控我们的基础架构，查看除了应用程序测试产生的负载之外，它还处理多少负载，并且我们可以确定是否存在争用。
*   自动化测试的好处是，我们可以将它们安排在稍后的时间点，并选择一个应用程序外部流量最少的时间。
*   我们可以使用提供良好隔离的基础设施。

关于最后一点，我喜欢本演示中使用的消息代理所采用的方法。EnMasse 从第一天起就以 OpenShift 为理念，可以按需产生新的专用代理。我们可以为试运行准备好它，然后让它退役。没有其他应用程序正在使用代理，它在测试运行后提供隔离和解除，确保保留最少的资源。监视代理还可以确保它不是负载下性能的限制因素。

### 测试自动化

对于测试自动化，我依赖于两个工具:

*   用于创建负载的 JMeter
*   詹金斯负责配器

这两个工具都可以利用 OpenShift，并在其上作为容器运行。我将在我的第三篇文章中演示如何做到这一点。

### JMeter

[![JMeter logo](../Images/f476af54d15ea746bf530e8bf89322cd.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc687f4a90f.png)

许多工具可以用来支持测试自动化，比如 JMeter、Gatling、Locust、Grinder 和 Tsung。它们提供了一种健壮的、可伸缩的、灵活的方法来产生测试负载。可以很容易地配置消息模板、测试数据集或负载注入模式。我喜欢 JMeter 的一个方面是可以使用它的 UI 来设计和实验测试，然后让它们从命令行运行，这对于高负载的预定测试是必须的。当我们需要在设计阶段与技术含量较低的人员进行互动，或者一旦技术方面的问题得到解决，让他们修改和细化测试用例时，UI 也有帮助。

与应用程序类似，将测试参数(消息数量、注入速率、持续时间等)外部化是有利的。)以便不同的测试用例可以一个接一个地运行，而无需人工干预或复制测试代码。

将 JMeter 作为一个容器运行使得它的设置易于移植和处理。OpenShift 还允许您控制分配给 JMeter 和应用程序的资源，并提供对更大资源池的访问。当测试完成后，两者都可以被处理掉。

可观察性也与 JMeter 相关，以确保它不受资源限制，也不是执行测试的限制因素；简单来说，就是保证温度计不坏。另一个方面是它也有助于在边界测量性能。当您使用 OpenTracing 时，仅仅知道应用程序处理一条消息需要多长时间是不够的；了解消息在被拾取之前在队列中等待了多长时间也很重要。测试 JMeter 可以提供一个更好的近似值。

### 詹金斯

[![Jenkins logo](../Images/7955149fa727f97cde0a552c57ccc147.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc6857d8e69.png)

为了运行自动化测试，最好有一个指挥。这就是詹金斯。它将自动化

*   创建
*   环境的供应
*   部署
*   用各种配置触发测试执行
*   清洁/退役

它还可以提供测试结果的高级视图:通过或失败。它还可以很好地与 JMeter 集成，以便快速查看趋势。通过添加从头构建和在测试结束时退出运行的功能，Jenkins 提供了信心，即所测试的是版本控制系统中预期和跟踪的内容。

Jenkins 允许我们按计划运行测试，和/或，例如，每当一个变更被提交到主干时。它是持续集成和持续交付的首选工具。它为使性能测试成为自动化非回归测试的一部分提供了必要的功能。

## 考虑

请注意以下注意事项。

### 避免浪费资源

性能测试通常需要大量的资源，因为最好在复制生产的环境中运行测试。通过利用 OpenShift 和 Jenkins 管道，可以在几分钟内创建一个测试运行时的环境，并在运行后立即将其退役。通过这样做，我们不需要在超出所需的时间内调动资源，这可能意味着能源和成本的显著节约。

### 可重复性

能够看到代码或配置变化对性能的影响，使我们能够理解设计或实现决策所做的权衡，并在我们对这些影响感到不舒服时做出快速反应。删除和重新创建方法为两次运行之间的比较提供了一个清晰的起点。此外，它还提供了信心，即被测试的内容也是存储库中可用和标记的内容(代码源、配置和容器注册)。

对于像 OpenShift 这样高度动态的平台，重要的是要确保在运行期间能够调动相同数量的资源，以便能够对它们进行比较。因此，我们需要用等于 CPU/内存限制的 CPU/内存请求来配置部署。我们不允许基于运行组件实例的节点的负载(由其他应用程序)的任何资源波动。这不同于我们在生产中可能有的情况，在生产中我们可能希望调动尽可能多的可用资源。

同样，如前所述，在代理级别、网络或文件系统限制争用也很重要。最好在测试期间监控代理、I/O 和 NIO。

### 资料组

延迟和吞吐量通常会受到正在处理的数据的显著影响。重要的是要有一个代表生产数据的数据集，可以在两次运行之间重用。至于功能测试中的差异，不仅需要考虑生产数据的多样性，还需要考虑特定数据集的出现。如果数据是敏感的，最好使用可能已经匿名的真实生产数据集。

在这方面，卡夫卡对它的“重放”能力非常感兴趣。然而，它的使用需要非常不同的架构设计和测试方法，这超出了本文的范围。

### 注入模式

[![Graphic depicting the need to be aware of the injection pattern](../Images/65e4e2a1a5bad84c8e4a3374d1d6fa82.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc67a437188.png)

了解生产环境中数据的注入模式对于创建重要的测试用例非常重要，因为延迟以及某种程度上的吞吐量都会受到它的影响。我们的应用程序是在处理一批消息还是一串消息？在一天、一分钟或一秒钟内有强烈的变化吗？每小时 180 万条消息不同于每分钟 30，000 条消息或每秒 500 条消息。均匀分布不是给定的。我曾经见过这样的系统，它在每小时均匀分布 180 万条消息的情况下运行得非常好，但是在现实生活中，由于批量注入，90%的消息都没有达到 SLA 的要求。注入消息只花了 20 分钟，而且每小时都在发生。

相反，另一个系统在测试中表现不佳。每秒钟大约注入 80 条消息，这是模拟组件的最快速度。将消息平均分布在后 90%的消息上满足了目标 SLA。顺便说一下，这是可以在 JMeter 中配置的。

如果是绿地应用，一旦在生产中积累了一些经验，就需要对注入速率进行假设并重新进行评估。

### 测量点

下图描述了组件如何应对负载。第一个显示了一个正在处理负载的组件；第二个显示的是一个无法处理负载的组件。

[![Graphic of a component that is coping with the load](../Images/ade814047181dc3265b16a5c490bc6e2.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc65e8c735d.png)

[![Graphic of a component that is not coping with the load](../Images/25e046d452a51b965f189e6a735f9a51.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc6624e300f.png)

要真正了解应用程序的性能，关注测量点是很重要的。当您查看使用代理的应用程序时，消息入队的时间比它被客户出队的时间更相关(而且，两者都提供有价值的信息)。当组件无法处理负载时，等待处理的消息所花费的时间受影响最大。上面的图表提供了一个思路:当组件不堪重负时，寻找花费在“排队”和“读取和处理”上的时间如何演变的一般模式。

Java 应用程序在内存消耗方面也有很多困惑。堆大小及其利用率只是一部分。元空间和线程堆栈(默认设置可能高达每线程 1MB)也可能占很大一部分。除此之外，还有系统级的内存，例如用于打开的文件/套接字。由于操作系统通过共享和缓存优化其使用的方式，系统级使用的内存并不容易计算。当应用程序在 OpenShift 上的容器内运行时，cgroups 报告的值就是要监控的值。

### 协调省略

协调省略的思想是被测系统的响应时间可能会影响测量。JMeter 配置了有限数量的线程，并且资源也有限。如果发送消息的调用被阻塞了更长的时间，这也意味着高延迟，这也可能会阻止 JMeter 在该时间间隔内发送目标数量的消息；因此，当系统表现不佳时，将导致获得较少的测量值。这将扭曲计算出的延迟百分比和平均值。该演示旨在验证应用程序(而非代理)的性能/行为。也有可能异步发布，因此是非阻塞的。考虑到这一点，我可以做出以下假设:

*   JMeter 能够产生请求的负载(由于 OpenShift 提供的可伸缩性)
*   代理能够及时接收消息(不会阻塞 JMeter)

关于协调省略和延迟测量的有趣资源在这里[和这里](http://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html)和[这里](https://www.youtube.com/watch?v=lJ8ydIuPFeU)可用。

## 性能调整

当报告测试结果时，有两个级别是很方便的。一个简单的级别，它仅通过趋势的高级视图来判断测试是通过还是失败，另一个级别，它可以利用可观察性，并为解决性能问题或性能下降提供有价值的信息。在几分钟内看到应用程序在测试中的表现是非常重要的。如果分析时间太长，团队就不会定期查看结果。当我们谈论性能时，我们通常从三个方面来看:

*   吞吐量
*   潜伏
*   资源(内存、CPU 等。)消费

### 应用阶段

[![A graphic of a question mark](../Images/d2a55057cfb264c1d01622cd25e53737.png)T2】](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc63520eef2.png)

关于性能调优，理解并决定我们想要优化什么以及我们准备在哪些方面做出妥协是很重要的。除了性能与资源消耗之外，根据所做的选择，应用程序在以下阶段的行为可能会有所不同:

*   启动
*   初始化
*   定态
*   最大负荷

过去，通常针对峰值负载进行调整，此时应用程序最难满足其 SLA。启动和初始化很少发生。转向一次性容器，选择可能不再那么明确了。考虑到自动扩展，集群重新平衡容器可能会更频繁地停止和启动。如果我们的应用程序实例需要几分钟才能启动，并且我们必须对每秒产生数千条消息的批处理的启动所产生的负载做出响应，那么自动伸缩的能力没有帮助。此外，启动后处理的第一条消息在预热阶段(JIT 编译和优化、池加载等)可能会有更长的等待时间。).

在进行测试时，预热应用程序是我们可能要考虑的事情。

就调优而言，需要做出一些决定和权衡，这将在不同阶段和启动时间影响吞吐量和延迟:

*   是否应该在启动时加载所有库？
*   启动时是否应该完全填充连接和其他池？
*   SubstrateVM 和 AOT 编译(以降低吞吐量为代价减少启动时间和内存消耗)会成为选择吗？

### 调谐提示

[![Graphic of several screws, where all but one are the same](../Images/7f26688ac7cdc8e152d384e7491d486a.png)](https://developers.redhat.com/blog/wp-content/uploads/2018/11/img_5bdc62d1a7ac8.png)

有几个螺丝可以调整，以提高性能。以下是本文中描述的应用程序类型的非详尽列表:

*   消息排序:保证消息排序可以防止并行性，或者需要重新排序，这通常会对性能产生很大影响。只有在严格要求时才应该这样做。通常值得研究在哪里可以放松部分处理。
*   预取策略:预取是一种非常有用的吞吐量优化。它防止应用程序线程在处理消息之前等待通过网络从代理获取消息。然而，对于非独占消费者，我们应该注意，一个实例不会使等待消息池挨饿；否则，我们可能会以一种有趣的模式结束，比如说，当实例 2 什么也不做时，实例 1 消耗并处理了多达 50 条消息(预取大小)，然后实例 2 消耗并处理了 50 条消息，而实例 1 什么也不做，依此类推。
*   线程争用:如果我们有一个多线程应用程序，我们需要注意线程处于阻塞或等待状态的时间。
*   上下文切换:更多的线程并不意味着更好的吞吐量。花费在上下文切换上的时间可能会显著影响性能。我们需要控制应用程序创建的线程数量。
*   池、内存保留和垃圾收集:这是一个很大的主题，我可以推荐[这篇文章](https://developers.redhat.com/blog/2014/07/15/dude-wheres-my-paas-memory-tuning-javas-footprint-in-openshift-part-1/)。还应该注意的是，与前面提到的要点相关的更改可能会影响对垃圾收集进行的任何调优。
*   I/O 和文件系统:出于几个原因，容器通常写入网络存储，而不是本地文件系统。这可能会引入额外的延迟。例如，可以考虑异步写入和缓存来减轻影响。

## 结论

感谢阅读这篇文章。我希望你觉得这第一部分很有趣。第二部分将介绍如何使用非常成功的开源项目轻松构建可观测性。第三部分也是最后一部分将深入研究测试自动化和编排。

## “利用 OpenShift 或 Kubernetes 进行自动化性能测试”系列中的所有文章

*   [第 1 部分:基本原理和方法](https://developers.redhat.com/blog/2018/11/22/automated-performance-testing-kubernetes-openshift/)(本文)
*   [第 2 部分:构建一个可观测性堆栈](https://developers.redhat.com/blog/2019/01/03/leveraging-openshift-or-kubernetes-for-automated-performance-tests-part-2/)
*   [第 3 部分:自动化测试和指标收集](https://developers.redhat.com/blog/2019/01/16/openshift-kubernetes-automated-performance-tests-part-3)

*Last updated: September 3, 2019*